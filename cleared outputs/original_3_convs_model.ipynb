{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "original_3_convs_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xsFqVZGWR0_"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "0xsFqVZGWR0_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdd1ec51"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import glob\n",
        "import os\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "# data_path = '/Users/ruchiraravishankar/Downloads/iam_dataset/'\n",
        "# data_path = '/content/drive/MyDrive/iam_dataset/'\n",
        "data_path = '/content/drive/MyDrive/iam_dataset/'\n",
        "save_path = '/content/drive/MyDrive/testing_proj/iam_dataset/saved_model'\n",
        "save_model = False\n",
        "save_results = False\n",
        "if not os.path.exists(save_path):\n",
        "  os.makedirs(save_path)"
      ],
      "id": "bdd1ec51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46779cd3"
      },
      "source": [
        "# path = '/Users/ruchiraravishankar/Downloads/iam_dataset/xml/*'\n",
        "# path = data_path + 'xml/*'\n",
        "# xml_path = glob.glob(path)\n",
        "# print(len(xml_path))"
      ],
      "id": "46779cd3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73eb14c4"
      },
      "source": [
        "# # for file in xml_path:\n",
        "# # file = xml_path[0]\n",
        "# # print(file)\n",
        "# d = {}\n",
        "# d_i = {}\n",
        "# def get_form_metadata(file):\n",
        "#     with open(file, 'r') as f:\n",
        "#         data = f.read()\n",
        "#     Bs_data = BeautifulSoup(data, \"lxml\")\n",
        "#     form = Bs_data.find('form')\n",
        "#     form_id = form.get('id')\n",
        "#     writer_id = form.get('writer-id')\n",
        "# #     print(form_id)\n",
        "# #     print(writer_id)\n",
        "#     d[writer_id] = form_id\n",
        "#     d_i[form_id] = writer_id\n",
        "# for file in xml_path:\n",
        "#     get_form_metadata(file)\n",
        "    \n",
        "# print(d)\n",
        "# print(d_i)\n"
      ],
      "id": "73eb14c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izX6koOJG82T"
      },
      "source": [
        "import pickle\n",
        "# a_file = open(\"/content/drive/MyDrive/iam_dataset/d.pkl\", \"wb\")\n",
        "# pickle.dump(d, a_file)\n",
        "# a_file.close()\n",
        "\n",
        "# b_file = open(\"/content/drive/MyDrive/iam_dataset/d_i.pkl\", \"wb\")\n",
        "# pickle.dump(d_i, b_file)\n",
        "# b_file.close()"
      ],
      "id": "izX6koOJG82T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn_d63WEH7Ct"
      },
      "source": [
        "print(os.listdir(os.curdir))"
      ],
      "id": "Xn_d63WEH7Ct",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2mwE6pWHFe7"
      },
      "source": [
        "\n",
        "# a_file = open(\"/content/drive/MyDrive/iam_dataset/d.pkl\", \"rb\")\n",
        "# d = pickle.load(a_file)\n",
        "# print(d)\n",
        "\n",
        "b_file = open(\"/content/drive/MyDrive/iam_dataset/d_i.pkl\", \"rb\")\n",
        "d_i = pickle.load(b_file)\n",
        "print(d_i)"
      ],
      "id": "N2mwE6pWHFe7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51dee1df"
      },
      "source": [
        "form_writer_list = list(d_i.items())"
      ],
      "id": "51dee1df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "798c9b82"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "top_writers = []\n",
        "num_writers = 50 # taking the top 50 writers\n",
        "writers_counter = Counter(d_i.values())\n",
        "for writer_id,_ in writers_counter.most_common(num_writers):\n",
        "    top_writers.append(writer_id)\n",
        "\n",
        "print(top_writers)"
      ],
      "id": "798c9b82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81d4b19d"
      },
      "source": [
        "top_forms = []\n",
        "for form_id, author_id in d_i.items():\n",
        "    if author_id in top_writers:\n",
        "        top_forms.append(form_id)\n",
        "print(top_forms)"
      ],
      "id": "81d4b19d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43e9a94d"
      },
      "source": [
        "# for key, value in sorted(top_forms.items()):\n",
        "#     print(key, len([item for item in value if item])) "
      ],
      "id": "43e9a94d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dd71b3d"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "\n",
        "# Create temp directory to save writers' forms in (assumes files have already been copied if the directory exists)\n",
        "temp_sentences_path = data_path + \"/temp_sentences\"\n",
        "if not os.path.exists(temp_sentences_path):\n",
        "    os.makedirs(temp_sentences_path)\n",
        "    # Copy forms that belong to the top 50 most common writers to the temp directory\n",
        "    original_sentences_path = data_path + \"/sentences/**/**/*.png\"\n",
        "    for file_path in glob.glob(original_sentences_path):\n",
        "        image_name = file_path.split('/')[-1]  \n",
        "        file_name, _ = os.path.splitext(image_name)\n",
        "        form_id = '-'.join(file_name.split('-')[0:2])\n",
        "        if form_id in top_forms:\n",
        "            shutil.copy2(file_path, temp_sentences_path + \"/\" + image_name)"
      ],
      "id": "5dd71b3d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21fabb85"
      },
      "source": [
        "img_files = np.zeros((0), dtype=str)\n",
        "print(img_files)\n",
        "img_targets = np.zeros((0), dtype=str)\n",
        "print(img_targets)\n",
        "sentence_dict = {}\n",
        "path_to_files = os.path.join(temp_sentences_path, '*')\n",
        "for file_path in glob.glob(path_to_files):\n",
        "    img_files = np.append(img_files, file_path)\n",
        "    file_name, _ = os.path.splitext(file_path.split('/')[-1]  )\n",
        "    form_id = '-'.join(file_name.split('-')[0:2])\n",
        "    for key in d_i:\n",
        "        if key == form_id:\n",
        "            img_targets = np.append(img_targets, d_i[form_id])\n",
        "#             sentence_dict[d_i[form_id]] = form_id"
      ],
      "id": "21fabb85",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a058f95b"
      },
      "source": [
        "print(img_files.shape)\n",
        "print(img_targets.shape)\n",
        "\n",
        "print(Counter(img_targets).keys())\n",
        "print(Counter(img_targets).values())"
      ],
      "id": "a058f95b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60c00257"
      },
      "source": [
        "sen_dict = {}\n",
        "\n",
        "for i in range(len(img_targets)):\n",
        "    if img_targets[i] not in sen_dict.keys():\n",
        "        sen_dict[img_targets[i]] = [img_files[i]]\n",
        "    else:\n",
        "        sen_dict[img_targets[i]].append(img_files[i])"
      ],
      "id": "60c00257",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f74f4d3"
      },
      "source": [
        "print(len(sen_dict['207']))"
      ],
      "id": "6f74f4d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dba0606a"
      },
      "source": [
        "# print(len(sen_dict.values()))\n",
        "number_of = []\n",
        "writer_ids = []\n",
        "for key, value in sorted(sen_dict.items()):\n",
        "    writer_ids.append(key)\n",
        "    number_of.append(len([item for item in value if item]))\n",
        "print(len(writer_ids))"
      ],
      "id": "dba0606a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86ed215f"
      },
      "source": [
        "# crop so all images are of same size\n",
        "# plot histogram \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "plt.figure(figsize=(40,10))\n",
        "plt.bar(writer_ids, number_of, width=0.5, color='g')\n",
        "plt.show()"
      ],
      "id": "86ed215f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e9dc457"
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.image as mpimg\n",
        "# %matplotlib inline\n",
        "# # img_files, img_targets\n",
        "# img_shape_list = []\n",
        "# for file_name in img_files:\n",
        "#     img = mpimg.imread(file_name)\n",
        "#     img_shape_list.append(img.shape)\n",
        "#     plt.figure(figsize = (10,10))\n",
        "#     plt.imshow(img, cmap ='gray')"
      ],
      "id": "9e9dc457",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4e101f5"
      },
      "source": [
        "# print(img_shape_list)"
      ],
      "id": "b4e101f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2412c1a"
      },
      "source": [
        "# arr = np.array(img_shape_list)\n",
        "# print(arr.shape)"
      ],
      "id": "b2412c1a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "711c826b"
      },
      "source": [
        "# print(arr.min(axis=0))"
      ],
      "id": "711c826b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f98edc00"
      },
      "source": [
        "for file_name in img_files[:2]:\n",
        "    img = mpimg.imread(file_name)\n",
        "    plt.figure(figsize = (10,10))\n",
        "    plt.imshow(img, cmap ='gray')\n",
        "    print(img.shape)"
      ],
      "id": "f98edc00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "decd44b1"
      },
      "source": [
        "# build a binary classifier on the writer id 000 and train on 0 and 1\n",
        "# do the images have to be the same size???\n",
        "# train test split => equal number of 0s and 1s (500 and 500)\n",
        "# 1000 data points -> 900 to train and 100 to test"
      ],
      "id": "decd44b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "810c0daf"
      },
      "source": [
        "writer_0_files = np.array(sen_dict['000'][:500])\n",
        "print(writer_0_files.shape)\n",
        "\n",
        "writer_0 = np.c_[writer_0_files, np.zeros(500)]\n",
        "\n",
        "print(writer_0.shape)"
      ],
      "id": "810c0daf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKaXGQOoE59J"
      },
      "source": [
        "print(writer_0[0])"
      ],
      "id": "uKaXGQOoE59J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7191f93b"
      },
      "source": [
        "labelled_data = np.stack([img_files, img_targets], axis=1)\n",
        "\n",
        "print(labelled_data.shape)"
      ],
      "id": "7191f93b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22206532"
      },
      "source": [
        "labelled_data_0 = labelled_data[labelled_data[:,1] == '000']\n",
        "\n",
        "print(labelled_data_0.shape)\n",
        "\n",
        "labelled_data_0 = labelled_data_0[:196]\n",
        "print(labelled_data_0.shape)\n",
        "\n",
        "print(np.unique(labelled_data_0[:,1]))"
      ],
      "id": "22206532",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "025913bd"
      },
      "source": [
        "remove_000 = labelled_data[labelled_data[:,1]!='000']\n",
        "\n",
        "print(np.unique(remove_000[:,1]))\n",
        "print(np.unique(remove_000[:,1]).shape)"
      ],
      "id": "025913bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "926e3c22"
      },
      "source": [
        "y = [remove_000[remove_000[:,1]==k] for k in np.unique(remove_000[:,1])]\n",
        "print(len(y))"
      ],
      "id": "926e3c22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "affcb9d8"
      },
      "source": [
        "labelled_data_1 = []\n",
        "for i in y:\n",
        "    labelled_data_1.append(i[:4])\n",
        "print(len(labelled_data_1))\n",
        "\n",
        "labelled_data_1 = np.array(labelled_data_1)\n",
        "\n",
        "print(labelled_data_1.shape)"
      ],
      "id": "affcb9d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "797dd98d"
      },
      "source": [
        "labelled_data_1=labelled_data_1.reshape(-1,2)"
      ],
      "id": "797dd98d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6c293dc"
      },
      "source": [
        "print(labelled_data_1.shape)"
      ],
      "id": "c6c293dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef54f007"
      },
      "source": [
        "print(np.unique(labelled_data_1[:,1]))"
      ],
      "id": "ef54f007",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72fb393e"
      },
      "source": [
        "labelled_data_new = np.concatenate((labelled_data_0, labelled_data_1))\n",
        "np.random.shuffle(labelled_data_new)\n",
        "print(labelled_data_new.shape) # mix of sentences from writer id 0 and others, labelled"
      ],
      "id": "72fb393e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ea68409"
      },
      "source": [
        "# split before augmenting to ensure the sentences are being split and not the augmented samples\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(labelled_data_new[:,0], labelled_data_new[:,1], test_size=0.33, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
        "print(X_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape)"
      ],
      "id": "2ea68409",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b41601b"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "CROP_SIZE = 113\n",
        "NUM_LABELS = 50\n",
        "\n",
        "def get_augmented_sample(sample, label, sample_ratio):\n",
        "    # Get current image details\n",
        "    img = Image.open(sample)\n",
        "    img_width = img.size[0]\n",
        "    img_height = img.size[1]\n",
        "\n",
        "    # Compute resize dimensions such that aspect ratio is maintained\n",
        "    height_fac = CROP_SIZE / img_height\n",
        "    size = (int(img_width * height_fac), CROP_SIZE)\n",
        "\n",
        "    # Resize image \n",
        "    new_img = img.resize((size), Image.ANTIALIAS)\n",
        "    new_img_width = new_img.size[0]\n",
        "    new_img_height = new_img.size[1]\n",
        "\n",
        "    # Generate a random number of crops of size 113x113 from the resized image\n",
        "    x_coord = list(range(0, new_img_width - CROP_SIZE))\n",
        "    num_crops = int(len(x_coord) * sample_ratio)\n",
        "    random_x_coord = random.sample(x_coord, num_crops)\n",
        "    \n",
        "    # Create augmented images (cropped forms) and map them to a label (writer)\n",
        "    images = []\n",
        "    labels = []\n",
        "    for x in random_x_coord:\n",
        "        img_crop = new_img.crop((x, 0, x + CROP_SIZE, CROP_SIZE))\n",
        "        # Transform image to an array of numbers\n",
        "        images.append(np.asarray(img_crop))\n",
        "        labels.append(label)\n",
        "\n",
        "    return images, labels"
      ],
      "id": "4b41601b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55dc6891"
      },
      "source": [
        "def augment(data, labels):\n",
        "    augmented_sample_list = []\n",
        "    augmented_label_list = []\n",
        "    for i in range(len(data)):\n",
        "        augmented_samples, augmented_labels = get_augmented_sample(data[i], labels[i],0.1)\n",
        "        augmented_sample_list.append(augmented_samples)\n",
        "        augmented_label_list.append(augmented_labels)\n",
        "    return augmented_sample_list, augmented_label_list"
      ],
      "id": "55dc6891",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "664668a5"
      },
      "source": [
        "augsamps, auglbls = augment(X_train, y_train) # returns two lists of arrays\n",
        "augsamps_test, auglbls_test = augment(X_test, y_test)\n",
        "augsamps_val, auglbls_val = augment(X_val, y_val)"
      ],
      "id": "664668a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f35824a"
      },
      "source": [
        "import operator\n",
        "from functools import reduce\n",
        "\n",
        "# flattening the lists\n",
        "aug_samps = reduce(operator.add, augsamps)\n",
        "aug_lbls = reduce(operator.add, auglbls)\n",
        "\n",
        "aug_samps_test = reduce(operator.add, augsamps_test)\n",
        "aug_lbls_test = reduce(operator.add, auglbls_test)\n",
        "\n",
        "aug_samps_val = reduce(operator.add, augsamps_val)\n",
        "aug_lbls_val = reduce(operator.add, auglbls_val)\n",
        "\n",
        "print(len(aug_samps))\n",
        "print(len(aug_lbls))\n",
        "\n",
        "print(len(aug_samps_test))\n",
        "print(len(aug_lbls_test))\n",
        "\n",
        "print(len(aug_samps_val))\n",
        "print(len(aug_lbls_val))"
      ],
      "id": "2f35824a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "967237ec"
      },
      "source": [
        "# converting lists to arrays\n",
        "aug_samps_arr = np.array(aug_samps)\n",
        "aug_lbls_arr = np.array(aug_lbls)\n",
        "aug_samps_arr_test = np.array(aug_samps_test)\n",
        "aug_lbls_arr_test = np.array(aug_lbls_test)\n",
        "aug_samps_arr_val = np.array(aug_samps_val)\n",
        "aug_lbls_arr_val = np.array(aug_lbls_val)\n",
        "\n",
        "print(aug_samps_arr.shape)\n",
        "print(aug_lbls_arr.shape)\n",
        "print(aug_samps_arr_test.shape)\n",
        "print(aug_lbls_arr_test.shape)\n",
        "print(aug_samps_arr_val.shape)\n",
        "print(aug_lbls_arr_val.shape)"
      ],
      "id": "967237ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f568281e",
        "scrolled": true
      },
      "source": [
        "X_train = aug_samps_arr\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, CROP_SIZE, CROP_SIZE)\n",
        "X_train = X_train.astype('float16')\n",
        "X_train /= 255.0\n",
        "y_train = aug_lbls_arr\n",
        "\n",
        "\n",
        "X_test = aug_samps_arr_test\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, CROP_SIZE, CROP_SIZE)\n",
        "X_test = X_test.astype('float16')\n",
        "X_test /= 255.0\n",
        "y_test = aug_lbls_arr_test\n",
        "\n",
        "X_val = aug_samps_arr_val\n",
        "X_val = X_val.reshape(X_val.shape[0], 1, CROP_SIZE, CROP_SIZE)\n",
        "X_val = X_val.astype('float16')\n",
        "X_val /= 255.0\n",
        "y_val = aug_lbls_arr_val\n",
        "\n"
      ],
      "id": "f568281e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7d2904a"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)"
      ],
      "id": "e7d2904a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78a69fdf"
      },
      "source": [
        "new = np.unique(y_train)\n",
        "print(new)\n",
        "\n",
        "y_train[y_train !='000'] = 1\n",
        "new = np.unique(y_train)\n",
        "print(new)\n",
        "\n",
        "y_train[y_train=='000'] = 0\n",
        "new = np.unique(y_train)\n",
        "print(new)\n",
        "\n",
        "print(y_train.shape)\n",
        "\n",
        "y_train = y_train.astype('float16')\n",
        "\n",
        "print(np.unique(y_train))"
      ],
      "id": "78a69fdf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c049e9bb"
      },
      "source": [
        "new = np.unique(y_test)\n",
        "print(new)\n",
        "\n",
        "y_test[y_test !='000'] = 1\n",
        "new = np.unique(y_test)\n",
        "print(new)\n",
        "\n",
        "y_test[y_test=='000'] = 0\n",
        "new = np.unique(y_test)\n",
        "print(new)\n",
        "\n",
        "print(y_test.shape)\n",
        "\n",
        "y_test = y_test.astype('float16')\n",
        "\n",
        "print(np.unique(y_test))"
      ],
      "id": "c049e9bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfDEMsWkdQmn"
      },
      "source": [
        "new = np.unique(y_val)\n",
        "print(new)\n",
        "\n",
        "y_val[y_val !='000'] = 1\n",
        "new = np.unique(y_val)\n",
        "print(new)\n",
        "\n",
        "y_val[y_val=='000'] = 0\n",
        "new = np.unique(y_val)\n",
        "print(new)\n",
        "\n",
        "print(y_val.shape)\n",
        "\n",
        "y_val = y_val.astype('float16')\n",
        "\n",
        "print(np.unique(y_val))"
      ],
      "id": "yfDEMsWkdQmn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1wfoJoZqOOb"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "CUDA = True\n",
        "device = torch.device('cuda' if CUDA else 'cpu')\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if CUDA else {}\n",
        "device"
      ],
      "id": "g1wfoJoZqOOb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09217c4b"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "tensor_x_train = torch.Tensor(X_train).type(torch.cuda.FloatTensor) # transform to torch tensor\n",
        "tensor_y_train = torch.Tensor(y_train).type(torch.cuda.FloatTensor).long()\n",
        "\n",
        "\n",
        "\n",
        "# tensor_x_train = torch.Tensor(X_train) # transform to torch tensor\n",
        "# tensor_y_train = torch.Tensor(y_train).long()\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x_train,tensor_y_train) # create your datset\n",
        "train_dataloader = DataLoader(train_dataset,batch_size=16, shuffle=True, drop_last=True) # create your dataloader"
      ],
      "id": "09217c4b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNDz-ofrfkNC"
      },
      "source": [
        "tensor_x_val = torch.Tensor(X_val).type(torch.cuda.FloatTensor) # transform to torch tensor\n",
        "tensor_y_val = torch.Tensor(y_val).type(torch.cuda.FloatTensor).long()\n",
        "# tensor_x_train = torch.Tensor(X_train) # transform to torch tensor\n",
        "# tensor_y_train = torch.Tensor(y_train).long()\n",
        "\n",
        "val_dataset = TensorDataset(tensor_x_val,tensor_y_val) # create your datset\n",
        "val_dataloader = DataLoader(val_dataset,batch_size=16, shuffle=True, drop_last=True) # create your dataloader"
      ],
      "id": "DNDz-ofrfkNC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c120a864"
      },
      "source": [
        "tensor_x_test = torch.Tensor(X_test).type(torch.cuda.FloatTensor) # transform to torch tensor\n",
        "tensor_y_test = torch.Tensor(y_test).long().type(torch.cuda.FloatTensor).long()\n",
        "\n",
        "# tensor_x_test = torch.Tensor(X_test) # transform to torch tensor\n",
        "# tensor_y_test = torch.Tensor(y_test).long()\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x_test,tensor_y_test) # create your datset\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, drop_last=True) # create your dataloader"
      ],
      "id": "c120a864",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOXYkXWgCKt7"
      },
      "source": [
        "tensor_x_train.shape"
      ],
      "id": "HOXYkXWgCKt7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63b0ecba"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "class Binary_Classifier(nn.Module): \n",
        "    def __init__(self):\n",
        "        super(Binary_Classifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=3)\n",
        "        self.conv_drop = nn.Dropout2d()\n",
        "        self.conv3 = nn.Conv2d(20, 30, kernel_size=3)\n",
        "        self.fc1 = nn.Linear(4320, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pool2d = nn.MaxPool2d(2)\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.max_pool2d(self.conv1(x)))\n",
        "        x = self.relu(self.max_pool2d(self.conv_drop(self.conv2(x))))\n",
        "        x = self.relu(self.max_pool2d(self.conv_drop(self.conv3(x))))\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        # print(x.shape)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.conv_drop(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "id": "63b0ecba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29d5c074"
      },
      "source": [
        "model=Binary_Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = 0.0001)\n",
        "\n",
        "# import torch.optim.lr_scheduler.StepLR\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n"
      ],
      "id": "29d5c074",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ykv4G0NtmH9"
      },
      "source": [
        "# train_losses = []\n",
        "train_loss_record = []\n",
        "valid_loss_record = []\n",
        "# save_path = \"/content/drive/MyDrive/iam_dataset/saved_model\""
      ],
      "id": "9Ykv4G0NtmH9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1olHEZqhudxC"
      },
      "source": [
        "from datetime import datetime\n",
        "train_log_string = '%s :: Epoch %i :: Iter %i / %i :: train loss: %0.4f'\n",
        "valid_log_string = '%s :: Epoch %i :: valid loss: %0.4f\\n'\n",
        "def train(epoch, best_record, criterion, optimizer):\n",
        "    print('Start Epoch {} Training...'.format(epoch))\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    train_loss_sum = []\n",
        "    for idx, (data, target) in enumerate(train_dataloader):\n",
        "        if torch.cuda.is_available():\n",
        "          data, target = data.cuda(), target.cuda()   \n",
        "        optimizer.zero_grad()\n",
        "        #  forward-pass\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        #backward-pass\n",
        "        loss.backward()\n",
        "        # Update lr scheduler\n",
        "        # scheduler.step()\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "        train_loss_sum.append(loss.item())\n",
        "        if ((idx +1) % 64) == 0:\n",
        "          print(train_log_string % (datetime.now(), epoch, idx + 1, len(train_dataloader), np.mean(train_loss)))\n",
        "          print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
        "          train_loss = []\n",
        "    # mean_tr = torch.mean(torch.stack(train_loss_sum))\n",
        "    mean_tr = np.mean(np.array(train_loss_sum))\n",
        "    print('Train Loss at epoch {}: {}\\n'.format(epoch, mean_tr))\n",
        "    train_loss_record.append(mean_tr)\n",
        "\n",
        "\n",
        "    print('Epoch {} Validating...'.format(epoch))\n",
        "\n",
        "    \n",
        "    valid_loss = []\n",
        "    \n",
        "    for idx, (data, target) in enumerate(val_dataloader):\n",
        "        output = model(data)\n",
        "        output = output.squeeze(1)\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss.append(loss.item())\n",
        "        mean_val = np.mean(np.array(valid_loss))\n",
        "        print(valid_log_string % (datetime.now(), epoch, mean_val))\n",
        "        valid_loss_record.append(mean_val)\n",
        "    if np.mean(np.array(valid_loss)) < best_record:\n",
        "        best_record = np.mean(np.array(valid_loss))\n",
        "        # save the best model\n",
        "        if save_model:\n",
        "          state_dict = {\n",
        "              'epoch': epoch,\n",
        "              'model': model.state_dict(),\n",
        "              'optimizer': optimizer.state_dict(),\n",
        "          }\n",
        "          torch.save(state_dict, save_path+'/model_3_convs.pt')\n",
        "          print('Model saved!\\n')\n",
        "        \n",
        "    print(\"DONE!\")\n",
        "    return train_loss_record, valid_loss_record, best_record"
      ],
      "id": "1olHEZqhudxC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba6w34bjtN9v"
      },
      "source": [
        "import time\n",
        "best = np.inf\n",
        "train(1,best,criterion,optimizer)\n",
        "torch.cuda.synchronize()\n",
        "times = []\n",
        "for epoch in range(15):\n",
        "    start_epoch = time.time()\n",
        "    tlr, vlr, best = train(epoch, best, criterion, optimizer)\n",
        "    torch.cuda.synchronize()\n",
        "    end_epoch = time.time()\n",
        "    elapsed = end_epoch - start_epoch\n",
        "    times.append(elapsed)\n",
        "\n",
        "\n",
        "avg_time = sum(times)/15"
      ],
      "id": "Ba6w34bjtN9v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training time 26.658623886108398\n",
        "print(avg_time)"
      ],
      "metadata": {
        "id": "SyGUB9tBR8h0"
      },
      "id": "SyGUB9tBR8h0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UD3CqceSgPs"
      },
      "source": [
        "# temp = [t.detach().cpu().numpy() for t in tlr]\n",
        "# arr = np.array(temp)\n",
        "# print(arr.shape)\n",
        "\n",
        "plt.plot(tlr)"
      ],
      "id": "8UD3CqceSgPs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnFjmaPqPwzH"
      },
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists(save_path +'/model_3_convs.pt'):\n",
        "    print('Loading checkpoint: %s' %save_path +'/model_3_convs.pt')\n",
        "    path = torch.load(save_path +'/model_3_convs.pt')\n",
        "    epoch = path['epoch']\n",
        "    model.load_state_dict(path['model'])\n",
        "    optimizer.load_state_dict(path['optimizer'])\n",
        "else:\n",
        "  print(\"DID NOT DO ANYTHING\")"
      ],
      "id": "qnFjmaPqPwzH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UElBlZoYQnpp"
      },
      "source": [
        "result = []\n",
        "prediction = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for idx, (data, target) in enumerate(test_dataloader):\n",
        "        output = model(data)\n",
        "        output_sm = nn.Softmax(dim=1)(output)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        prediction.append(predicted)\n",
        "        result.append(output_sm)\n",
        "        \n",
        "result = torch.stack(result).detach().cpu().numpy()\n",
        "prediction = torch.stack(prediction).detach().cpu().numpy()\n",
        "\n",
        "print(result.shape, prediction.shape)"
      ],
      "id": "UElBlZoYQnpp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs1Q0JC2xKjs"
      },
      "source": [
        "print(prediction.reshape(-1).shape)"
      ],
      "id": "Hs1Q0JC2xKjs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCcQqc-VsfnE"
      },
      "source": [
        "import pandas as pd\n",
        "res = pd.DataFrame(result.reshape(-1,2))\n",
        "pred = pd.DataFrame(prediction.reshape(-1))\n",
        "\n",
        "# res.to_csv(\"/content/drive/MyDrive/iam_dataset/saved_model/results_conv_3.csv\", header=False, index=False)\n",
        "# pred.to_csv(\"/content/drive/MyDrive/iam_dataset/saved_model/pred_conv_3.csv\", header=False, index=False)"
      ],
      "id": "dCcQqc-VsfnE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGZJv2Di1QSz"
      },
      "source": [
        "print(res.shape, pred.shape)"
      ],
      "id": "sGZJv2Di1QSz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRodoZGT0iIY"
      },
      "source": [
        "y_test_new = y_test[:result.reshape(-1,2).shape[0]]"
      ],
      "id": "iRodoZGT0iIY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgdEBEj41lsO"
      },
      "source": [
        "print(y_test_new.shape)"
      ],
      "id": "rgdEBEj41lsO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaJkMnicswF_"
      },
      "source": [
        "result_df = pd.DataFrame({'ground truth': y_test_new, 'NN output': prediction.reshape(-1), 'probs 0': res[0], 'probs 1': res[1]})\n",
        "result_df"
      ],
      "id": "iaJkMnicswF_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYHr-O7ny7Ii"
      },
      "source": [
        "print(result_df)\n",
        "# result_df.to_csv(\"/content/drive/MyDrive/iam_dataset/saved_model/result_df_conv_3.csv\")"
      ],
      "id": "QYHr-O7ny7Ii",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COaorBTG-iwa"
      },
      "source": [
        "# work out the accuracy and other metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "actual = result_df['ground truth']\n",
        "predicted = result_df['NN output']\n",
        "# confusion matrix\n",
        "matrix = confusion_matrix(actual, predicted, labels=[0,1])\n",
        "print('Confusion matrix : \\n',matrix)\n",
        "\n",
        "# outcome values order in sklearn\n",
        "tp, fn, fp, tn = confusion_matrix(actual, predicted, labels=[0,1]).reshape(-1)\n",
        "print('Outcome values : \\n', tp, fn, fp, tn)\n",
        "\n",
        "# classification report for precision, recall f1-score and accuracy\n",
        "matrix_2 = classification_report(actual,predicted,labels=[1,0])\n",
        "print('Classification report : \\n',matrix_2)"
      ],
      "id": "COaorBTG-iwa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rp6mAJJU2xK"
      },
      "source": [
        "import seaborn as sn\n",
        "df_cm = pd.DataFrame(matrix, index = [i for i in \"01\"],\n",
        "                  columns = [i for i in \"01\"])\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True)"
      ],
      "id": "7Rp6mAJJU2xK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0eM31vqtXR4"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "matrix = classification_report(actual,predicted,labels=[0,1], output_dict=True)\n",
        "print('Classification report : \\n',matrix)\n",
        "accuracy_score(actual, predicted)"
      ],
      "id": "C0eM31vqtXR4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFa8Ur5Ytecn"
      },
      "source": [
        "matrix_df = pd.DataFrame(matrix).transpose()\n",
        "if save_results:\n",
        "  matrix_df.to_csv(\"/content/drive/MyDrive/iam_dataset/saved_model/classification_report_conv_3_node3_pi.csv\", header=True, index=True)\n",
        "\n"
      ],
      "id": "lFa8Ur5Ytecn",
      "execution_count": null,
      "outputs": []
    }
  ]
}