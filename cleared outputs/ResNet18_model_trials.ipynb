{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet18 model trials.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zixHmePc7HQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H9hV6PEIKBD"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# save_path = '/content/drive/MyDrive/iam_dataset/saved_model/resnet/'\n",
        "# data_path = '/content/drive/MyDrive/iam_dataset/'\n",
        "\n",
        "data_path = '/content/drive/MyDrive/testing_proj/iam_dataset/'\n",
        "save_path = '/content/drive/MyDrive/testing_proj/iam_dataset/saved_model/resnet/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# toggles\n",
        "pre_trained = True\n",
        "train_on_node1 = False\n",
        "pre_trained_on_node1_use_on_node2 = False\n",
        "\n",
        "\n",
        "save_results = False\n",
        "save_model = False"
      ],
      "metadata": {
        "id": "EdsLVb9-cyqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFo7g_nQKehH"
      },
      "source": [
        "labelled_data = np.load(data_path + 'labelled_data.npy')\n",
        "\n",
        "print(labelled_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnD3OCIVLzck"
      },
      "source": [
        "labelled_data_0 = labelled_data[labelled_data[:,1] == '000']\n",
        "\n",
        "print(labelled_data_0.shape)\n",
        "\n",
        "if pre_trained_on_node1_use_on_node2:\n",
        "  labelled_data_0 = labelled_data_0[196:(196*2)]\n",
        "else:\n",
        "  labelled_data_0 = labelled_data_0[:196]\n",
        "print(labelled_data_0.shape)\n",
        "\n",
        "print(np.unique(labelled_data_0[:,1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0fpGL7iL0YX"
      },
      "source": [
        "remove_0 = labelled_data[labelled_data[:,1]!='000']\n",
        "\n",
        "print(np.unique(remove_0[:,1]))\n",
        "print(np.unique(remove_0[:,1]).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o4oibXrL62P"
      },
      "source": [
        "y = [remove_0[remove_0[:,1]==k] for k in np.unique(remove_0[:,1])]\n",
        "print(len(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvE4cYAxMCSW"
      },
      "source": [
        "labelled_data_1 = []\n",
        "for i in y:\n",
        "  if pre_trained_on_node1_use_on_node2:\n",
        "    labelled_data_1.append(i[5:9])\n",
        "  else:\n",
        "    labelled_data_1.append(i[:4])\n",
        "\n",
        "\n",
        "print(len(labelled_data_1))\n",
        "\n",
        "labelled_data_1 = np.array(labelled_data_1)\n",
        "\n",
        "print(labelled_data_1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNhdAdmsMJPX"
      },
      "source": [
        "labelled_data_1=labelled_data_1.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbzkxU6mMMgw"
      },
      "source": [
        "print(np.unique(labelled_data_1[:,1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EQb0WwaOfCM"
      },
      "source": [
        "labelled_data_new = np.concatenate((labelled_data_0, labelled_data_1))\n",
        "np.random.shuffle(labelled_data_new)\n",
        "print(labelled_data_new.shape) # mix of sentences from writer id 0 and others, labelled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5zsmUv08q13"
      },
      "source": [
        "# split before augmenting to ensure the sentences are being split and not the augmented samples\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(labelled_data_new[:,0], labelled_data_new[:,1], test_size=0.33, random_state=42)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jupnqls-Lc1K"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "CROP_SIZE = 113\n",
        "NUM_LABELS = 50\n",
        "\n",
        "def get_augmented_sample(sample, label, sample_ratio):\n",
        "    # Get current image details\n",
        "    img = Image.open(sample)\n",
        "    img_width = img.size[0]\n",
        "    img_height = img.size[1]\n",
        "\n",
        "    # Compute resize dimensions such that aspect ratio is maintained\n",
        "    height_fac = CROP_SIZE / img_height\n",
        "    size = (int(img_width * height_fac), CROP_SIZE)\n",
        "\n",
        "    # Resize image \n",
        "    new_img = img.resize((size), Image.ANTIALIAS)\n",
        "    new_img_width = new_img.size[0]\n",
        "    new_img_height = new_img.size[1]\n",
        "\n",
        "    # Generate a random number of crops of size 113x113 from the resized image\n",
        "    x_coord = list(range(0, new_img_width - CROP_SIZE))\n",
        "    num_crops = int(len(x_coord) * sample_ratio)\n",
        "    random_x_coord = random.sample(x_coord, num_crops)\n",
        "    \n",
        "    # Create augmented images (cropped forms) and map them to a label (writer)\n",
        "    images = []\n",
        "    labels = []\n",
        "    for x in random_x_coord:\n",
        "        img_crop = new_img.crop((x, 0, x + CROP_SIZE, CROP_SIZE))\n",
        "        # Transform image to an array of numbers\n",
        "        images.append(np.asarray(img_crop))\n",
        "        labels.append(label)\n",
        "\n",
        "    return images, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YwS9_bRPk7t"
      },
      "source": [
        "def augment(data, labels):\n",
        "    augmented_sample_list = []\n",
        "    augmented_label_list = []\n",
        "    for i in range(len(data)):\n",
        "        augmented_samples, augmented_labels = get_augmented_sample(data[i], labels[i],0.1)\n",
        "        augmented_sample_list.append(augmented_samples)\n",
        "        augmented_label_list.append(augmented_labels)\n",
        "    return augmented_sample_list, augmented_label_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8wIUyX6P5PW"
      },
      "source": [
        "augsamps, auglbls = augment(X_train, y_train) # returns two lists of arrays\n",
        "ausamps_test, auglbls_test = augment(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iClV7pgP7wB"
      },
      "source": [
        "import operator\n",
        "from functools import reduce\n",
        "\n",
        "# flattening the lists\n",
        "aug_samps = reduce(operator.add, augsamps)\n",
        "aug_lbls = reduce(operator.add, auglbls)\n",
        "\n",
        "aug_samps_test = reduce(operator.add, ausamps_test)\n",
        "aug_lbls_test = reduce(operator.add, auglbls_test)\n",
        "print(len(aug_samps))\n",
        "print(len(aug_lbls))\n",
        "\n",
        "print(len(aug_samps_test))\n",
        "print(len(aug_lbls_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gbvaci01QHgj"
      },
      "source": [
        "# converting lists to arrays\n",
        "aug_samps_arr = np.array(aug_samps)\n",
        "aug_lbls_arr = np.array(aug_lbls)\n",
        "aug_samps_arr_test = np.array(aug_samps_test)\n",
        "aug_lbls_arr_test = np.array(aug_lbls_test)\n",
        "\n",
        "print(aug_samps_arr.shape)\n",
        "print(aug_lbls_arr.shape)\n",
        "print(aug_samps_arr_test.shape)\n",
        "print(aug_lbls_arr_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkpYIfNsQM7x"
      },
      "source": [
        "X_train = aug_samps_arr\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, CROP_SIZE, CROP_SIZE)\n",
        "X_train = X_train.astype('float16')\n",
        "X_train /= 255.0\n",
        "y_train = aug_lbls_arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSre0ptbDSAn"
      },
      "source": [
        "X_test = aug_samps_arr_test\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, CROP_SIZE, CROP_SIZE)\n",
        "X_test = X_test.astype('float16')\n",
        "X_test /= 255.0\n",
        "y_test = aug_lbls_arr_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WeIMmB5Q675"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyWREsrWQ7f-"
      },
      "source": [
        "new = np.unique(y_train)\n",
        "print(new)\n",
        "\n",
        "y_train[y_train !='000'] = 1\n",
        "new = np.unique(y_train)\n",
        "print(new)\n",
        "\n",
        "y_train[y_train=='000'] = 0 # 0 is 0\n",
        "new = np.unique(y_train)\n",
        "print(new)\n",
        "\n",
        "print(y_train.shape)\n",
        "\n",
        "y_train = y_train.astype('float16')\n",
        "\n",
        "print(np.unique(y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz9HBySE9F8w"
      },
      "source": [
        "new = np.unique(y_test)\n",
        "print(new)\n",
        "\n",
        "y_test[y_test !='000'] = 1\n",
        "new = np.unique(y_test)\n",
        "print(new)\n",
        "\n",
        "y_test[y_test=='000'] = 0\n",
        "new = np.unique(y_test)\n",
        "print(new)\n",
        "\n",
        "print(y_test.shape)\n",
        "\n",
        "y_test = y_test.astype('float16')\n",
        "\n",
        "print(np.unique(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSGlwYkgROON"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "CUDA = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if CUDA else 'cpu')\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if CUDA else {}\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ2ch9Y1RHlP"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "tensor_x_train = torch.Tensor(X_train).type(torch.cuda.FloatTensor) # transform to torch tensor\n",
        "tensor_y_train = torch.Tensor(y_train).type(torch.cuda.FloatTensor).long()\n",
        "\n",
        "# tensor_x_train = torch.Tensor(X_train) # transform to torch tensor\n",
        "# tensor_y_train = torch.Tensor(y_train).long()\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x_train,tensor_y_train) # create your datset\n",
        "train_dataloader = DataLoader(train_dataset,batch_size=64, shuffle=True, drop_last=True) # create your dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhGnrY37CUtT"
      },
      "source": [
        "tensor_x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c120a864"
      },
      "source": [
        "tensor_x_test = torch.Tensor(X_test).type(torch.cuda.FloatTensor) # transform to torch tensor\n",
        "tensor_y_test = torch.Tensor(y_test).long().type(torch.cuda.FloatTensor).long()\n",
        "\n",
        "# tensor_x_test = torch.Tensor(X_test) # transform to torch tensor\n",
        "# tensor_y_test = torch.Tensor(y_test).long()\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x_test,tensor_y_test) # create your datset\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, drop_last=True) # create your dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXltzoDaKs97"
      },
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "if pre_trained:\n",
        "  model = models.resnet18(pretrained=True)\n",
        "else:\n",
        "  model = models.resnet18(pretrained=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIgcafI6K5Ao"
      },
      "source": [
        "# change first conv to take 1 channel instead of 3\n",
        "print(model.conv1)\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False).to(device)\n",
        "print(model.conv1)\n",
        "# num_ftrs = model.fc.in_features\n",
        "# model.fc = nn.Linear(num_ftrs, 2).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx0Q7PFqRfUC"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
        "model.to(device)\n",
        "print(next(model.parameters()).device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPOdL6yVZF66"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMiXAng53sex"
      },
      "source": [
        "if pre_trained or pre_trained_on_node1_use_on_node2:\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb4llj5HX9pb"
      },
      "source": [
        "if pre_trained or pre_trained_on_node1_use_on_node2:\n",
        "  for param in model.layer4.parameters():\n",
        "    param.requires_grad = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etgpNS8sZ_Wv"
      },
      "source": [
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_dcto_cRix5"
      },
      "source": [
        "if pre_trained_on_node1_use_on_node2:\n",
        "  if os.path.exists(save_path + 'trained_on_node1.pt'):\n",
        "      print('Loading checkpoint: %s' % save_path + 'trained_on_node1.pt')\n",
        "      path = torch.load(save_path + 'trained_on_node1.pt')\n",
        "      # epoch = path['epoch']\n",
        "      model.load_state_dict(path['model'])\n",
        "      optimizer.load_state_dict(path['optimizer'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ0BW0cQZ5YD"
      },
      "source": [
        "for param in model.parameters():\n",
        "  print(param.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzIBsfe0Rhn9"
      },
      "source": [
        "train_losses = []\n",
        "train_loss_record = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxLydxjs5KcU"
      },
      "source": [
        "from datetime import datetime\n",
        "train_log_string = '%s :: Epoch %i :: Iter %i / %i :: train loss: %0.4f'\n",
        "def train(epoch):\n",
        "    print('Start Epoch {} Training...'.format(epoch))\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    train_loss_sum = []\n",
        "    for idx, (data, target) in enumerate(train_dataloader):\n",
        "        # data = data.to(device)\n",
        "        # target = target.to(device)\n",
        "        # print(data.shape)\n",
        "        optimizer.zero_grad()\n",
        "        #  forward-pass\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        #backward-pass\n",
        "        loss.backward()\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "        train_loss.append(loss.data.cpu().numpy())\n",
        "        train_loss_sum.append(loss)\n",
        "        if ((idx +1) % 5000) == 0:\n",
        "          print(train_log_string % (datetime.now(), epoch, idx + 1, len(train_dataloader), np.mean(train_loss)))\n",
        "          train_loss = []\n",
        "    mean_tr = torch.mean(torch.stack(train_loss_sum))\n",
        "    print('Train Loss at epoch {}: {}\\n'.format(epoch, mean_tr))\n",
        "    train_loss_record.append(mean_tr)\n",
        "    return train_loss_record"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9zEsKwqHqfP"
      },
      "source": [
        "for epoch in range(10):\n",
        "    tlr = train(epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pnn_-LhH6a5"
      },
      "source": [
        "tlr_cpu = [i.cpu().detach().numpy() for i in tlr]\n",
        "\n",
        "plt.plot(tlr_cpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Cb2uBRtR1xm"
      },
      "source": [
        "result = []\n",
        "prediction = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for idx, (data, target) in enumerate(test_dataloader):\n",
        "        output = model(data)\n",
        "        output_sm = nn.Softmax(dim=1)(output)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        prediction.append(predicted)\n",
        "        result.append(output_sm)\n",
        "        \n",
        "result = torch.stack(result).detach().cpu().numpy()\n",
        "prediction = torch.stack(prediction).detach().cpu().numpy()\n",
        "\n",
        "print(result.shape, prediction.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5dX2nuDR53u"
      },
      "source": [
        "print(prediction.reshape(-1).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37exoPXrI8MY"
      },
      "source": [
        "print(result.reshape(-1,2).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpA4J_P9Hkpj"
      },
      "source": [
        "# save trained model\n",
        "state_dict = {\n",
        "    # 'epoch': epoch,\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "}\n",
        "if save_model:\n",
        "  if pre_trained_on_node1_use_on_node2:\n",
        "    torch.save(state_dict, save_path + 'pretrained_on_node1_reuse_node2.pt')\n",
        "  elif pre_trained:\n",
        "    torch.save(state_dict, save_path + 'pretrained_on_imagenet.pt')\n",
        "  else:\n",
        "    torch.save(state_dict, save_path + 'trained_on_node1.pt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSYdwlA2R_uT"
      },
      "source": [
        "import pandas as pd\n",
        "res = pd.DataFrame(result.reshape(-1,2))\n",
        "pred = pd.DataFrame(prediction.reshape(-1))\n",
        "\n",
        "# res.to_csv(save_path + \"results_pretrained_on_node1_reuse_node2.csv\", header=True, index=True)\n",
        "# pred.to_csv(save_path + \"pred_pretrained_on_node1_reuse_node2.csv\", header=True, index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVZ6jQRzSKuk"
      },
      "source": [
        "print(res.shape, pred.shape)\n",
        "\n",
        "y_test_new = y_train[:result.reshape(-1,2).shape[0]]\n",
        "\n",
        "print(y_test_new.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ4jq1cRUGVs"
      },
      "source": [
        "result_df = pd.DataFrame({'ground truth': y_test_new, 'NN output': prediction.reshape(-1), 'probs 0': res[0], 'probs 1': res[1]})\n",
        "result_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7n1CqexUX_K"
      },
      "source": [
        "# work out the accuracy and other metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "actual = result_df['ground truth']\n",
        "predicted = result_df['NN output']\n",
        "# confusion matrix\n",
        "matrix = confusion_matrix(actual, predicted, labels=[0,1])\n",
        "print('Confusion matrix : \\n',matrix)\n",
        "\n",
        "# outcome values order in sklearn\n",
        "tp, fn, fp, tn = confusion_matrix(actual, predicted, labels=[0,1]).reshape(-1)\n",
        "print('Outcome values : \\n', tp, fn, fp, tn)\n",
        "\n",
        "# classification report for precision, recall f1-score and accuracy\n",
        "matrix_2 = classification_report(actual,predicted,labels=[1,0])\n",
        "print('Classification report : \\n',matrix_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQJefgJeUaS3"
      },
      "source": [
        "import seaborn as sn\n",
        "df_cm = pd.DataFrame(matrix, index = [i for i in \"01\"],\n",
        "                  columns = [i for i in \"01\"])\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyzmJwdaUckf"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "matrix = classification_report(actual,predicted,labels=[0,1], output_dict=True)\n",
        "print('Classification report : \\n',matrix)\n",
        "accuracy_score(actual, predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVFYL_OzUpOX"
      },
      "source": [
        "matrix_df = pd.DataFrame(matrix).transpose()\n",
        "if save_results:\n",
        "  if pre_trained_on_node1_use_on_node2:\n",
        "    matrix_df.to_csv(save_path + \"classification_report_pretrained_on_node1_reuse_node2.csv\", header=False, index=False)\n",
        "  elif pre_trained:\n",
        "    matrix_df.to_csv(save_path + \"classification_report_pretrained_on_imagenet.csv\", header=False, index=False)\n",
        "  else:\n",
        "    matrix_df.to_csv(save_path + \"classification_report_trained_on_node1.csv\", header=False, index=False)\n",
        "\n",
        "# pd.to_csv(\"/content/drive/MyDrive/iam_dataset/saved_model/classification_report.txt\",matrix)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}